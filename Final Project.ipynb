{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1-eTO-SWj-"
      },
      "source": [
        "## Important packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uBp8eJ5wn4fm"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Pandas is required to read the data.\n",
        "# For some reason pyspark can't read the csv file correctly\n",
        "# So we have to read using pandas and then convert to spark DF\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.ml.feature import IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql.functions import when, col, regexp_replace, concat, lit, length\n",
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "from pyspark.ml.classification import NaiveBayesModel, NaiveBayes\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "33jp03OHVhPS"
      },
      "outputs": [],
      "source": [
        "def evaluate(df, labelCol = \"label\", predCol = \"prediction\"):\n",
        "    TP = df.filter((col(labelCol) == 0) & (col(predCol) == 0)).count()\n",
        "    FN = df.filter((col(labelCol) == 1) & (col(predCol) == 0)).count()\n",
        "    FP = df.filter((col(labelCol) == 0) & (col(predCol) == 1)).count()\n",
        "    TN = df.filter((col(labelCol) == 1) & (col(predCol) == 1)).count()\n",
        "\n",
        "    precision = (TP)/(TP+FP)\n",
        "    recall = (TP)/(TP+FN)\n",
        "    print(\"Accuracy: %.3f\" % float((TP+TN)/(TP+TN+FP+FN)))\n",
        "    print(\"Recall: %.3f\" % float(recall))\n",
        "    print(\"Precision: %.3f\" % float(precision))\n",
        "    print(\"F1 Score: %.3f\" % float(2*(precision * recall)/(precision +recall)))\n",
        "\n",
        "    (df\n",
        "        .crosstab('label','prediction')\n",
        "        .withColumnRenamed(\"label_prediction\", \"label\\prediction\")\n",
        "        .orderBy(\"label\\prediction\", asceding = False)\n",
        "        .show()\n",
        "    )\n",
        "\n",
        "    return ([[TP, FP], [FN, TN]], precision, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_VoOgp5S51X"
      },
      "source": [
        "## Spark Session \\& Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SJyOb7_9_uVo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"my_app_name\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://DESKTOP-OPVQFS1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>my_app_name</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x1ecae818370>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fake news data from CSV into a DataFrame\n",
        "data_path = r\"D:\\fake_news_workout\\news.csv\"\n",
        "spark_df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove unimportant rows of the df\n",
        "\n",
        "spark_df = spark_df.filter((spark_df.label == 'FAKE') | (spark_df.label == 'REAL'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Assuming 'label' is the name of the column containing the labels\n",
        "string_indexer = StringIndexer(inputCol='label', outputCol='encoded_label')\n",
        "spark_df = string_indexer.fit(spark_df).transform(spark_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1545"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-----+-------------+\n",
            "|                 _c0|               title|                text|label|encoded_label|\n",
            "+--------------------+--------------------+--------------------+-----+-------------+\n",
            "|                  95|‘Britain’s Schind...|A Czech stockbrok...| REAL|          1.0|\n",
            "|                1571|Trump takes on Cr...|Killing Obama adm...| REAL|          1.0|\n",
            "|             Oh well| I guess you real...| in that case all...| FAKE|          0.0|\n",
            "|“I’m going to be ...| I won’t be invol...|         ” he said.\"| REAL|          1.0|\n",
            "|                  So| after tough prim...| both parties are...| REAL|          1.0|\n",
            "|“I am the least r...|” he said at the ...|       believe me.”\"| REAL|          1.0|\n",
            "|                1787|GOP insiders: Car...|On this day in 19...| REAL|          1.0|\n",
            "|                9324|Mike Pence Drapes...|Trump Raises Conc...| FAKE|          0.0|\n",
            "|                 587|Senate race ranki...|The move would ma...| REAL|          1.0|\n",
            "|— OakTown ☢MAGA O...| 2016 This is a m...| Pug Lover & Game...| FAKE|          0.0|\n",
            "|This post is part...| an independent b...| a Washington thi...| REAL|          1.0|\n",
            "|                5937|3 Effects of Subs...|Drug and substanc...| FAKE|          0.0|\n",
            "|                7277|Tree Shaped Verti...|By Amanda Froelic...| FAKE|          0.0|\n",
            "|                5521|New Comment Featu...|\"Be the First to ...| FAKE|          0.0|\n",
            "|                9360|Ying and Yang (th...|Ying and Yang (th...| FAKE|          0.0|\n",
            "|                6646|Police Turn In Ba...|It should be evid...| FAKE|          0.0|\n",
            "|Despite his lead ...| Trump said he wi...|\"\" he said. \"\"But...| REAL|          1.0|\n",
            "|                1834|Biden makes anoth...|On this day in 19...| REAL|          1.0|\n",
            "|Andrew P. Napolitano| a former judge o...| is the senior ju...| REAL|          1.0|\n",
            "|                6197|Is google and You...|Is google and You...| FAKE|          0.0|\n",
            "+--------------------+--------------------+--------------------+-----+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries for text cleaning\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+----+-----+-------------+\n",
            "|_c0|title|text|label|encoded_label|\n",
            "+---+-----+----+-----+-------------+\n",
            "|  0|    0|   0|    0|            0|\n",
            "+---+-----+----+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cheking for null values\n",
        "\n",
        "spark_df.select([count(when(isnan(col), col)).alias(col) for col in spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-KwNJlUTVJj"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCeamMcTXlS"
      },
      "source": [
        "### Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUnPTcsTdk3",
        "outputId": "37d18d52-f025-455f-9130-12dbf9513e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "906\n",
            "+--------------------+-------------+-----+\n",
            "|           full_text|encoded_label|label|\n",
            "+--------------------+-------------+-----+\n",
            "|Trump Raises Conc...|          0.0|  0.0|\n",
            "|The Manhattan bil...|          1.0|  1.0|\n",
            "|John Oliver’s Sme...|          0.0|  0.0|\n",
            "| he said What the...|          1.0|  1.0|\n",
            "| she explains why...|          0.0|  0.0|\n",
            "| Trump will have ...|          1.0|  1.0|\n",
            "|Email If you can’...|          0.0|  0.0|\n",
            "+--------------------+-------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Deleting all rows that are missing text\n",
        "# df_rmv_nan_text = spark_df.filter(col(\"text\") != \"NaN\")\n",
        "df_rmv_nan_text = spark_df.filter(length(col(\"text\")) > 60)\n",
        "\n",
        "# There are a lot of NaN in the dataset.\n",
        "# Those are Null values in pandas that were\n",
        "# Converted to NaN string in spark df.\n",
        "# Since it is a string, it will not be recognized by na() methods\n",
        "# So, we have to manually change their value:\n",
        "df_no_nan = (df_rmv_nan_text\n",
        "             .withColumn(\"title\", when(col(\"title\") == \"NaN\", \" \")\n",
        "                                            .otherwise(col(\"title\")))\n",
        "             )\n",
        "\n",
        "\n",
        "## NOTE: Later on we will use Tokenizer from PySpark MLlib. This tokenizer\n",
        "##       takes care of converting all characters to lowercase, so it is\n",
        "##       not required in this step.\n",
        "\n",
        "# Remove non-character from title and text\n",
        "df_clean = (df_no_nan\n",
        "\n",
        "                 ## Removing any non-character from title\n",
        "                .withColumn(\"title\", \n",
        "                            regexp_replace(\n",
        "                                col('title'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "                \n",
        "                ## Removing any non-character from text\n",
        "                .withColumn(\"text\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "                \n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"text\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "                \n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"title\", \n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "                )\n",
        "\n",
        "\n",
        "# Concatenation of title and text when title doesn't appear in text\n",
        "df_combined = (df_clean\n",
        "                    .withColumn('full_text',\n",
        "                                  when(col(\"text\").contains(\n",
        "                                                    concat(col(\"title\"))),\n",
        "                                                    col(\"text\"))\n",
        "                                  \n",
        "                                  .otherwise(concat(col(\"title\"),\n",
        "                                                    lit(\" \"),\n",
        "                                                    col(\"text\"))))\n",
        "                    .select([\"full_text\",\"encoded_label\"])\n",
        "                    .withColumn(\"label\", col(\"encoded_label\").cast(DoubleType()))\n",
        "                    .dropDuplicates()\n",
        "                )\n",
        "\n",
        "\n",
        "# Clean memory             \n",
        "del df_rmv_nan_text, df_no_nan, df_clean\n",
        "\n",
        "# Sanity Check\n",
        "print(df_combined.count())\n",
        "df_combined.show(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9YAncjjgHV8"
      },
      "source": [
        "### Check Class Balance\n",
        "\n",
        "Still balanced!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZRUdo-tr9MG",
        "outputId": "8b2b379e-eba3-4d66-9ed7-a65fcbf847f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|  547|\n",
            "|  1.0|  359|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_combined.groupby(\"label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkusjNkLmrg5"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7dYGkSo5IE",
        "outputId": "09b6456b-95c4-435d-d1a4-e6116b8304d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "try:\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "\n",
        "# Sanity Check\n",
        "stopwords_ls[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HHWDngVPsm"
      },
      "source": [
        "### Stemmer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vJNIKC-uuSDr"
      },
      "outputs": [],
      "source": [
        "####################### Code Citation #######################\n",
        "# Author: Clare S. Y. Huang\n",
        "# Date: 01 Aug 2020\n",
        "# Title: Custom Transformer that can be fitted into Pipeline\n",
        "# URL: https://csyhuang.github.io/2020/08/01/custom-transformer/\n",
        "#############################################################\n",
        "\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "\n",
        "class Stemmer(Transformer, \n",
        "                 HasInputCol, \n",
        "                 HasOutputCol,\n",
        "                 DefaultParamsReadable, \n",
        "                 DefaultParamsWritable):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        super(Stemmer, self).__init__()\n",
        "        kwargs = self._input_kwargs\n",
        "        self.set_params(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def set_params(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        kwargs = self._input_kwargs\n",
        "        self._set(**kwargs)\n",
        "    \n",
        "    def get_input_col(self):\n",
        "        return self.getOrDefault(self.inputCol)\n",
        "\n",
        "    def get_output_col(self):\n",
        "        return self.getOrDefault(self.outputCol)\n",
        "\n",
        "    def _transform(self, df):\n",
        "\n",
        "        # Input and output column\n",
        "        input_col = self.get_input_col()\n",
        "        output_col = self.get_output_col()\n",
        "\n",
        "        # Initialize stemmer from nltk package\n",
        "        ps = PorterStemmer()\n",
        "        \n",
        "        # User Defined Function: stemming every word in the input column\n",
        "        transform_udf = F.udf(lambda x: [ps.stem(word) for word in x], ArrayType(StringType(), False))\n",
        "\n",
        "        # Return the new df with the new column\n",
        "        return df.withColumn(output_col, transform_udf(input_col))\n",
        "\n",
        "# Sanity check\n",
        "# words = Tokenizer(inputCol=\"text\", outputCol=\"words\").transform(spark_df)\n",
        "# test = Stem(inputCol = \"words\", outputCol = \"test\").transform(words)\n",
        "# test.select([\"words\", \"test\"]).show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwHHeQEpSOWG"
      },
      "source": [
        "## Dummy Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_8RGzpp14a2",
        "outputId": "93121b25-89eb-4fc3-ceaa-2f38b750f5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "|  1.0|       0.0|\n",
            "|  0.0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 7 rows\n",
            "\n",
            "Accuracy: 0.604\n",
            "Recall: 0.604\n",
            "Precision: 1.000\n",
            "F1 Score: 0.753\n",
            "+----------------+---+\n",
            "|label\\prediction|0.0|\n",
            "+----------------+---+\n",
            "|             0.0|547|\n",
            "|             1.0|359|\n",
            "+----------------+---+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([[547, 0], [359, 0]], 1.0, 0.6037527593818984)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dummy = (df_combined\n",
        "                .withColumn(\"prediction\", \n",
        "                    when((col(\"full_text\") == \"NaN\") , 1.0)\n",
        "                    .otherwise(0.0))\n",
        "                .withColumn(\"label\", col(\"label\").cast(FloatType()))\n",
        "                .select([\"label\",\"prediction\"])\n",
        "            )\n",
        "\n",
        "df_dummy.show(7)\n",
        "\n",
        "# Sanity Check\n",
        "evaluate(df_dummy, predCol = \"prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew9QgSIg4xO9"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sparknlp\n",
            "  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting spark-nlp (from sparknlp)\n",
            "  Downloading spark_nlp-5.3.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.1/57.1 kB 2.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sparknlp) (1.26.4)\n",
            "Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
            "Downloading spark_nlp-5.3.1-py2.py3-none-any.whl (564 kB)\n",
            "   ---------------------------------------- 0.0/564.8 kB ? eta -:--:--\n",
            "   --------------------------------- ------ 471.0/564.8 kB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 564.8/564.8 kB 7.1 MB/s eta 0:00:00\n",
            "Installing collected packages: spark-nlp, sparknlp\n",
            "Successfully installed spark-nlp-5.3.1 sparknlp-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sparknlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSJKOy1lHgFV"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-OcobknvOhr8"
      },
      "outputs": [],
      "source": [
        "# Split data to train and test\n",
        "train, test = df_combined.randomSplit([0.7,0.3], seed=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline # pipeline to transform data\n",
        "from pyspark.sql import SparkSession # to initiate spark\n",
        "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
        "from pyspark.ml.feature import HashingTF, IDF # vectorizer\n",
        "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
        "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
        "from pyspark.ml.classification import LogisticRegression # ml model\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # for hyperparameter tuning\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # to evaluate the model\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics # # performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|           full_text|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert sentences to list of words\n",
        "tokenizer = RegexTokenizer(inputCol=\"full_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "train_df = tokenizer.transform(train)\n",
        "train_df.select(['label','full_text', 'words']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|           full_text|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  1.0| 32 patients were...|[32, patients, we...|\n",
            "|  0.0| Brian Dobson is ...|[brian, dobson, i...|\n",
            "|  1.0| Charlie Hebdo ha...|[charlie, hebdo, ...|\n",
            "|  0.0| Churkin stated T...|[churkin, stated,...|\n",
            "|  1.0| Cruz was on cons...|[cruz, was, on, c...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert sentences to list of words\n",
        "tokenizer = RegexTokenizer(inputCol=\"full_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "test_df = tokenizer.transform(test)\n",
        "test_df.select(['label','full_text', 'words']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|[2006, 203, link,...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|[2016, johnghendy...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|[2016, poll, marg...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|[28, states, dist...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|[allan, served, t...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filter\")\n",
        "\n",
        "train_df= stopwords_remover.transform(train_df)\n",
        "\n",
        "train_df.select(['label','full_text', 'words', 'filter']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  1.0| 32 patients were...|[32, patients, we...|[32, patients, di...|\n",
            "|  0.0| Brian Dobson is ...|[brian, dobson, i...|[brian, dobson, w...|\n",
            "|  1.0| Charlie Hebdo ha...|[charlie, hebdo, ...|[charlie, hebdo, ...|\n",
            "|  0.0| Churkin stated T...|[churkin, stated,...|[churkin, stated,...|\n",
            "|  1.0| Cruz was on cons...|[cruz, was, on, c...|[cruz, constant, ...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filter\")\n",
        "\n",
        "test_df = stopwords_remover.transform(test_df)\n",
        "\n",
        "test_df.select(['label','full_text', 'words', 'filter']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|[2006, 203, link,...|(10000,[168,3469,...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|[2016, johnghendy...|(10000,[263,671,8...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|[2016, poll, marg...|(10000,[42,808,83...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|[28, states, dist...|(10000,[120,132,1...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|[allan, served, t...|(10000,[379,407,6...|\n",
            "|  0.0| American voters ...|[american, voters...|[american, voters...|(10000,[3712,5633...|\n",
            "|  0.0| Bill Clinton is ...|[bill, clinton, i...|[bill, clinton, s...|(10000,[47,157,16...|\n",
            "|  1.0| Brockway said of...|[brockway, said, ...|[brockway, said, ...|(10000,[855,1241,...|\n",
            "|  0.0| Bruce Dixon is m...|[bruce, dixon, is...|[bruce, dixon, ma...|(10000,[551,695,1...|\n",
            "|  1.0| Buffett said a G...|[buffett, said, a...|[buffett, said, g...|(10000,[1287,1780...|\n",
            "|  1.0| CNNsJake Tapper ...|[cnnsjake, tapper...|[cnnsjake, tapper...|(10000,[756,1280,...|\n",
            "|  1.0| Catherine Herrid...|[catherine, herri...|[catherine, herri...|(10000,[1402,1998...|\n",
            "|  0.0| Choose Your Plat...|[choose, your, pl...|[choose, platform...|(10000,[2913,5700...|\n",
            "|  1.0| Clinton said in ...|[clinton, said, i...|[clinton, said, s...|(10000,[756,4320,...|\n",
            "|  1.0| Clinton surrogat...|[clinton, surroga...|[clinton, surroga...|(10000,[756,1011,...|\n",
            "|  0.0| Courage Grows St...|[courage, grows, ...|[courage, grows, ...|(10000,[406,581,6...|\n",
            "|  1.0| DC dedicated to ...|[dc, dedicated, t...|[dc, dedicated, a...|(10000,[310,2432,...|\n",
            "|  1.0| Dan Gallo and Ja...|[dan, gallo, and,...|[dan, gallo, jaso...|(10000,[726,1402,...|\n",
            "|  1.0| David Weigel and...|[david, weigel, a...|[david, weigel, k...|(10000,[585,1402,...|\n",
            "|  1.0| Del The number o...|[del, the, number...|[del, number, pho...|(10000,[1562,1626...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate term frequency in each article\n",
        "hashing_tf = HashingTF(inputCol=\"filter\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "featurized_data = hashing_tf.transform(train_df)\n",
        "\n",
        "# TF-IDF vectorization of articles\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_vectorizer = idf.fit(featurized_data)\n",
        "train_df = idf_vectorizer.transform(featurized_data)\n",
        "\n",
        "train_df.select(\"label\",'full_text', 'words', 'filter', \"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|[2006, 203, link,...|(10000,[168,3469,...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|[2016, johnghendy...|(10000,[263,671,8...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|[2016, poll, marg...|(10000,[42,808,83...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|[28, states, dist...|(10000,[120,132,1...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|[allan, served, t...|(10000,[379,407,6...|\n",
            "|  0.0| American voters ...|[american, voters...|[american, voters...|(10000,[3712,5633...|\n",
            "|  0.0| Bill Clinton is ...|[bill, clinton, i...|[bill, clinton, s...|(10000,[47,157,16...|\n",
            "|  1.0| Brockway said of...|[brockway, said, ...|[brockway, said, ...|(10000,[855,1241,...|\n",
            "|  0.0| Bruce Dixon is m...|[bruce, dixon, is...|[bruce, dixon, ma...|(10000,[551,695,1...|\n",
            "|  1.0| Buffett said a G...|[buffett, said, a...|[buffett, said, g...|(10000,[1287,1780...|\n",
            "|  1.0| CNNsJake Tapper ...|[cnnsjake, tapper...|[cnnsjake, tapper...|(10000,[756,1280,...|\n",
            "|  1.0| Catherine Herrid...|[catherine, herri...|[catherine, herri...|(10000,[1402,1998...|\n",
            "|  0.0| Choose Your Plat...|[choose, your, pl...|[choose, platform...|(10000,[2913,5700...|\n",
            "|  1.0| Clinton said in ...|[clinton, said, i...|[clinton, said, s...|(10000,[756,4320,...|\n",
            "|  1.0| Clinton surrogat...|[clinton, surroga...|[clinton, surroga...|(10000,[756,1011,...|\n",
            "|  0.0| Courage Grows St...|[courage, grows, ...|[courage, grows, ...|(10000,[406,581,6...|\n",
            "|  1.0| DC dedicated to ...|[dc, dedicated, t...|[dc, dedicated, a...|(10000,[310,2432,...|\n",
            "|  1.0| Dan Gallo and Ja...|[dan, gallo, and,...|[dan, gallo, jaso...|(10000,[726,1402,...|\n",
            "|  1.0| David Weigel and...|[david, weigel, a...|[david, weigel, k...|(10000,[585,1402,...|\n",
            "|  1.0| Del The number o...|[del, the, number...|[del, number, pho...|(10000,[1562,1626...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate term frequency in each article\n",
        "hashing_tf = HashingTF(inputCol=\"filter\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "featurized_data_test = hashing_tf.transform(test_df)\n",
        "\n",
        "# TF-IDF vectorization of articles\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_vectorizer = idf.fit(featurized_data_test)\n",
        "test_df = idf_vectorizer.transform(featurized_data)\n",
        "\n",
        "test_df.select(\"label\",'full_text', 'words', 'filter', \"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "train=train_df.select(\"label\",\"features\")\n",
        "test=test_df.select(\"label\",\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "train =train.withColumn('label', train['label'].cast('double'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['features'], outputCol='dense_features')\n",
        "train = assembler.transform(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            features|      dense_features|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|(10000,[379,407,6...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[label: double, dense_features: vector]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.drop(\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost.spark import SparkXGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\retech-01\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-06 17:00:25,956 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
            "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "2024-03-06 17:00:36,087 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
          ]
        }
      ],
      "source": [
        "model=SparkXGBClassifier(label_col=\"label\").fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_df=model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "|label|            features|       rawPrediction|prediction|         probability|\n",
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  0.0|(10000,[263,671,8...|[2.95089197158813...|       0.0|[0.95030564069747...|\n",
            "|  1.0|(10000,[42,808,83...|[-0.8122609853744...|       1.0|[0.30740892887115...|\n",
            "|  1.0|(10000,[120,132,1...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  0.0|(10000,[379,407,6...|[1.32028520107269...|       0.0|[0.78922915458679...|\n",
            "|  0.0|(10000,[3712,5633...|[0.72081977128982...|       0.0|[0.67278754711151...|\n",
            "|  0.0|(10000,[47,157,16...|[2.24892854690551...|       0.0|[0.90455806255340...|\n",
            "|  1.0|(10000,[855,1241,...|[-0.3804846704006...|       1.0|[0.40601003170013...|\n",
            "|  0.0|(10000,[551,695,1...|[-1.2882757186889...|       1.0|[0.21614480018615...|\n",
            "|  1.0|(10000,[1287,1780...|[-2.2285096645355...|       1.0|[0.09721934795379...|\n",
            "|  1.0|(10000,[756,1280,...|[-2.8396818637847...|       1.0|[0.05521714687347...|\n",
            "|  1.0|(10000,[1402,1998...|[-1.8213135004043...|       1.0|[0.13927632570266...|\n",
            "|  0.0|(10000,[2913,5700...|[0.30015498399734...|       0.0|[0.57448041439056...|\n",
            "|  1.0|(10000,[756,4320,...|[-3.3191328048706...|       1.0|[0.03492063283920...|\n",
            "|  1.0|(10000,[756,1011,...|[-1.4631378650665...|       1.0|[0.18798786401748...|\n",
            "|  0.0|(10000,[406,581,6...|[-0.4396409094333...|       1.0|[0.39182657003402...|\n",
            "|  1.0|(10000,[310,2432,...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  1.0|(10000,[726,1402,...|[-1.9470452070236...|       1.0|[0.12487590312957...|\n",
            "|  1.0|(10000,[585,1402,...|[-2.2755310535430...|       1.0|[0.09316980838775...|\n",
            "|  1.0|(10000,[1562,1626...|[-1.1665059328079...|       1.0|[0.23748719692230...|\n",
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9494047619047623\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predict_df)\n",
        "\n",
        "# Print the AUC score\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GBT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features_vector\")\n",
        "from pyspark.ml.classification import GBTClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9442197610751611\n"
          ]
        }
      ],
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features_vector\", maxIter=10)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# Train the GBTClassifier model\n",
        "model_gbt = pipeline.fit(train_df)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = model_gbt.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the AUC score\n",
        "print(\"AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
